#####################################################################
# Sockeye-recipes Hyperparameter configuration file                 #
#                                                                   #
# Overview:                                                         #
# - "workdir" corresponds a group of preprocessed bitext and models #
#    for a given dataset. Each "workdir" can contain multiple       #
#    "datadir" and "modeldir" if desired                            #
# - "datadir" stores the BPE-preprocessed training and validation   #
#    bitext files                                                   #
# - "modeldir" is generated by Sockeye and stores all training info #
# - "rootdir" is path to your installation of sockeye-recipes,      #
#    e.g. ~/src/sockeye-recipes                                     #
#                                                                   #
# preprocess-bpe.sh:                                                #
# - input: Tokenized bitext for training ("train_tok") and          #
#   and validation ("valid_tok")                                    #
# - output: BPE-preprocessed bitext ("train_bpe", "valid_bpe")      #
#   and vocabulary ("bpe_vocab_src", "bpe_vocab_trg")               #
# - main hyperparameters: number of BPE symbols for source & target #
#                                                                   #
# train.sh:                                                         #
# - input: BPE-preprocessed bitext ("train_bpe", "valid_bpe")       #
# - output: "modeldir", which contains all training info and can    #
#    be used to translate                                           #
# - main hyperparameters: many! see below                           #
#                                                                   #
# translate.sh:                                                     #
# - input: this hyperparam file, which specifies modeldir           #
# - output: resulting target translation of source file             #
#####################################################################


#####################################################################
# (0) General settings (to be modified for each project)            #
#####################################################################

### User-specified directories ###
workdir=$HOME/sockeye-recipes/egs/pretrained_embeddings
modeldir=$workdir/models/model_loss_2.0/
rootdir=$HOME/sockeye-recipes
echo "ROOTDIR $rootdir"
# DESCRIPTION: rs1: RNN-based seq2seq model, Small

### Language pair (source and target) ###
# Note: We assume all bitext files contain these as suffices. 
# e.g. $train_tok.$src, $train_tok.$trg refer to the source and target 
src=src.aner
trg=dst.aner

### Tokenized training and validation data ###
# Note we assume tokenization is already done, and will only run BPE
# For tokenization and other preprocessing, see preprocess-tokenize.sh
# which does not use this hyperparam.txt file
train_tok=$workdir/data/newsela_Zhang_Lapata_splits/V0V4_V1V4_V2V4_V3V4_V0V3_V0V2_V1V3.aner.ori.train
valid_tok=$workdir/data/newsela_Zhang_Lapata_splits/V0V4_V1V4_V2V4_V3V4_V0V3_V0V2_V1V3.aner.ori.valid

train_src=$train_tok.$src
train_trg=$train_tok.$trg
valid_src=$valid_tok.$src
valid_trg=$valid_tok.$trg

embeddings_src=$workdir/data/emb/glove.6B.300d.txt
embeddings_trg=$workdir/data/emb/glove.6B.300d.txt

tokens_src=50000
tokens_trg=50000

#####################################################################
# (2) train.sh settings (modify if needed)                          #
#####################################################################

# Model architecture
num_embed="300:300"
rnn_num_hidden=256
rnn_attention_type="dot"
num_layers=2
rnn_cell_type="lstm"

# Regularization
embed_dropout=".0:.0"
rnn_dropout_inputs=".2:.2"
rnn_dropout_states=".2:.2"
label_smoothing=0.0

# Vocabulary
num_words="${tokens_src}:${tokens_trg}"
word_min_count="3:3"
max_seq_len="85:85"

# Training configuration
batch_size=86
optimizer=adam
initial_learning_rate=0.001
learning_rate_reduce_factor=0.7
#loss="simple-cross-entropy"
loss="cross-entropy"
complexity_file=$HOME/sockeye-recipes/new_scripts/change_loss/complexity_predictions.pkl
complexity_weight=2.0
seed=13

# Logging and stopping condition
checkpoint_frequency=4000
min_num_epochs=1
max_num_epochs=30
max_updates=500000
keep_last_params=5
decode_and_evaluate=1

